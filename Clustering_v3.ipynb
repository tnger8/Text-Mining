{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>trade</td>\n",
       "      <td>brazil anti inflation plan limps to anniversar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>crude</td>\n",
       "      <td>diamond shamrock dia cuts crude prices diamond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>crude</td>\n",
       "      <td>opec may have to meet to firm prices analysts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>crude</td>\n",
       "      <td>texaco canada cuts crude prices canadian cts b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>crude</td>\n",
       "      <td>texaco canada txc lowers crude postings texaco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class                                               Text\n",
       "15  trade  brazil anti inflation plan limps to anniversar...\n",
       "43  crude  diamond shamrock dia cuts crude prices diamond...\n",
       "55  crude  opec may have to meet to firm prices analysts ...\n",
       "76  crude  texaco canada cuts crude prices canadian cts b...\n",
       "77  crude  texaco canada txc lowers crude postings texaco..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this workshop we perform document clustering using sklearn\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# We are using the subnews dataset that we used last week. \n",
    "# The \"Class\" labels here are only used for sanity check of the clusters found later.\n",
    "# Remember, in actual use of document clustering, the documents DON'T come with labeled classes.\n",
    "# It's unsupervised learning.\n",
    "\n",
    "import pandas as pd\n",
    "news=pd.read_table('r8-train-all-terms.txt',header=None,names = [\"Class\", \"Text\"])\n",
    "subnews=news[(news.Class==\"trade\")| (news.Class=='crude')|(news.Class=='money-fx') ]\n",
    "subnews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the similar preprocessing we used last week.\n",
    "# The output of each document is a list of tokens.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "mystopwords=stopwords.words(\"English\") + ['one', 'become', 'get', 'make', 'take']\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "def pre_process(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens=[ WNlemma.lemmatize(t.lower()) for t in tokens]\n",
    "    tokens=[ t for t in tokens if t not in mystopwords]\n",
    "    tokens = [ t for t in tokens if len(t) >= 3 ]\n",
    "    text_after_process=\" \".join(tokens)\n",
    "    return(text_after_process)\n",
    "\n",
    "# Apply preprocessing to every document in the training set.\n",
    "text = subnews['Text']\n",
    "toks = text.apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(710, 2500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tfidf matrix\n",
    "vectorizer = TfidfVectorizer(max_df=0.7, max_features=2500,\n",
    "                             min_df=3, stop_words=mystopwords,\n",
    "                             use_idf=True)\n",
    "X = vectorizer.fit_transform(toks)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVD to reduce dimensions\n",
    "svd = TruncatedSVD(300)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "X_lsa = lsa.fit_transform(X)\n",
    "\n",
    "#set to False to perform inplace row normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 85%\n"
     ]
    }
   ],
   "source": [
    "# Check how much variance is explained\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=1000,\n",
       "    n_clusters=3, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=4321, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the actual clustering\n",
    "from sklearn.cluster import KMeans\n",
    "#random_state=4321\n",
    "km3 = KMeans(n_clusters=3, init='k-means++', max_iter=1000, n_init=1)\n",
    "%time km3.fit(X_lsa)\n",
    "\n",
    "#‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.\n",
    "#n_init : int, default: 10. Number of time the k-means algorithm will be run with different centroid seeds.\n",
    "#Maximum number of iterations of the k-means algorithm for a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient for 3 clusters: 0.050\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "labels = subnews['Class']\n",
    "\n",
    "# Silhouette: more similar within clusters, more distant between clusters\n",
    "# The higher the better (-1 to 1)\n",
    "\n",
    "print(\"Silhouette Coefficient for 3 clusters: %0.3f\"\n",
    "      % metrics.silhouette_score(X_lsa, km3.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: oil crude price opec barrel dlrs mln bpd company ecuador\n",
      "Cluster 1: stg mln money bank england market revised assistance shortage forecast\n",
      "Cluster 2: trade billion japan bank exchange dollar currency rate japanese would\n"
     ]
    }
   ],
   "source": [
    "# We still need to see the more representative words for each cluster to understand them.\n",
    "\n",
    "def print_terms(cm, num):\n",
    "    original_space_centroids = svd.inverse_transform(cm.cluster_centers_)\n",
    "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(num):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()\n",
    "\n",
    "print_terms(km3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's map the cluster label to the categories to see where is the confusion\n",
    "\n",
    "dict = {0: 'crude', 1: 'money-fx', 2: 'trade'}\n",
    "cluster_labels = [ dict[c] for c in km3.labels_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[246   0   0]\n",
      " [  1  52   0]\n",
      " [  6 154 251]]\n",
      "0.7732394366197183\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      crude       0.97      1.00      0.99       246\n",
      "   money-fx       0.25      0.98      0.40        53\n",
      "      trade       1.00      0.61      0.76       411\n",
      "\n",
      "avg / total       0.93      0.77      0.81       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(metrics.confusion_matrix(cluster_labels, labels))\n",
    "print(np.mean(cluster_labels == labels) )\n",
    "print(metrics.classification_report(cluster_labels, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
